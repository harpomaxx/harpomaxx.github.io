<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>overfitting on Computer Science Notes</title>
		<link>https://harpomaxx.github.io/tags/overfitting/</link>
		<description>Recent content in overfitting on harpomaxx.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Â© Copyright [Harpo MAxx] MIT LICENSE</copyright>
		<lastBuildDate>Sun, 04 Sep 2022 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="https://harpomaxx.github.io/tags/overfitting/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Beyond Random Split for Assessing Statistical Model Performance</title>
			<link>https://harpomaxx.github.io/post/beyond-random-split-for-assessing-statistical-model-performance/</link>
			<pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/beyond-random-split-for-assessing-statistical-model-performance/</guid>
			<description>
				
				Sometimes  the standard splitting techniques used for testing your machine learning models can underestimate the generalization performance of the model. In this  post, I expose some of the most common approaches for splitting your data beyond the classical random split approach. [5min read]
				
			</description>
		</item>
		<item>
			<title>Double Descent in Deep Learning</title>
			<link>https://harpomaxx.github.io/post/double-descent-in-deep-learning/</link>
			<pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/double-descent-in-deep-learning/</guid>
			<description>
				
				The U-shape observed when measuring model performance on testset as a function of its flexibility does not hold during training deep learning models. (WHAT!!!?????. Is the world going mad?. Not really.)
				
			</description>
		</item>
		
	</channel>
</rss>
