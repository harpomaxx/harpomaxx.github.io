<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.101.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Harpo MAxx"><meta name="keywords" content="cross-validation,dimensional reduction,reproducible Research,resampling,hastie"><meta name="description" content="From time to time you will need to compare the distribution of two datasets. There are plenty of information about this topic in statistics books and all over the Internet. In this post I discuss three very practical approaches coming from different perspectives. [3.5 min read](updated 04/01/2021)"><meta property="og:title" content="Three Common Ways for Comparing Two Dataset Distributions" />
<meta property="og:description" content="From time to time you will need to compare the distribution of two datasets. There are plenty of information about this topic in statistics books and all over the Internet. In this post I discuss three very practical approaches coming from different perspectives. [3.5 min read](updated 04/01/2021)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harpomaxx.github.io/post/comparing-two-sample-distribution/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2020-11-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-16T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Three Common Ways for Comparing Two Dataset Distributions"/>
<meta name="twitter:description" content="From time to time you will need to compare the distribution of two datasets. There are plenty of information about this topic in statistics books and all over the Internet. In this post I discuss three very practical approaches coming from different perspectives. [3.5 min read](updated 04/01/2021)"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/all.css" />
	<link rel="stylesheet" href="https://harpomaxx.github.io/css/katex.min.css" crossorigin="anonymous">
	<script defer src="https://harpomaxx.github.io/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="https://harpomaxx.github.io/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Three Common Ways for Comparing Two Dataset Distributions | Computer Science Notes</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/styles/default.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</head>
<body><header>
	
	<div id="avatar">
		<a href="https://harpomaxx.github.io/">
			<img src="/img/harpo-2.jpg" alt="Computer Science Notes">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://harpomaxx.github.io/">Computer Science Notes</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href="https://harpomaxx.github.io">CS Notes</a> is a simple blog to keep track about CS-related stuff I consider useful.</p><div id=social>
			<nav>
				<ul><li><a href="https://github.com/harpomaxx/"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="https://www.twitter.com/harpolabs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.researchgate.net/profile/Carlos_Catania"><i title="Research Gate" class="icons fab fa-researchgate"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="post-image">
		<img src="https://www.dropbox.com/s/zv97kxlwa95o7yn/histo.png?dl=1" >
	</div>
	
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">16</span>
				<span class="rest">Nov 2020</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Three Common Ways for Comparing Two Dataset Distributions</h1>
		</div>
	</div>
	<div class="author">
	
	
		<p>
		
		by <a href="https://twitter.com/harpomaxx">Harpo MAxx</a>
		
	
	
		(3.5 min read)
	
		</p>
	
	</div>

	<div class="markdown">
		
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>There are several situations where you will need to compare the distribution of two samples. A common situation is when you need to sample from a larger dataset in order to reduce the time required for hyper-parameters tuning. In this case, you want the sample follows the same distribution of the original dataset (population) to guarantee the tuned model works properly in production. Other possible scenario is when you want to compare the distribution between your training and testing sets to confirm the required level of similarity between both of them. In both cases the problem arises when you need to compare two samples with multiple predictors (features). For those cases there are several options to analyze and compare the distribution of any pair of data sets.</p>
<div id="the-visualization-approach." class="section level3">
<h3>The Visualization approach.</h3>
<p>Visual tools are the usual way for have a glimpse of your dataset . <a href="https://www.r-graph-gallery.com/histogram.html">Histograms</a> for instance come handy to visually compare two distributions when you have only one random variable. In the case of multiple variables, you can try other visual tools such as <a href="https://www.r-graph-gallery.com/boxplot.html">boxplot</a> or <a href="https://www.r-graph-gallery.com/violin.html">violin</a> charts. An alternative approach consists of performing a <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> on each dataset and plot the 2D projections using the two first principal components (PC). If you are lucky and the 2 first PC explains a considerable amount of the variability of your data, then PCA could be an very useful tool for providing you with a detailed view of the variability in both datasets. See the figure below for an example of the comparison of two very different datasets for training and testing.</p>
<figure>
<img src="/post/2020-11-17-comparing-two-sample-distribution.en_files/pca-boxplot-dissmilar.png" alt="sampling" width="100%"/>
<figcaption>
<h6>
Figure 1. A 2D projection of two different datasets considering the first 2 principal components. In addition, the figure includes the boxplot distribution for each predictor variable in both datasets.
</h6>
</figcaption>
</figure>
</div>
<div id="the-statistical-approach." class="section level3">
<h3>The Statistical Approach.</h3>
<p>Answering the question whether two samples have the same distribution is a task that can be resolved applying statistical tests. The common tests used for comparing two distribution are <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi squared</a> for categorical variables and <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a> (K-S) for numerical variables.</p>
<p>For a multiple variable dataset, the approach is quite simple: for each variable from the sample you should compare its probability distribution with the probability distribution of the same variable of the population. The result of the K-S test can be used in conjunction with information about <a href="https://explained.ai/rf-importance/#:~:text=The%20advantage%20of%20Random%20Forests,to%20the%20feature%20importance%20function.&amp;text=Notice%20that%20the%20function%20does,dividing%20by%20the%20standard%20deviation">variables importance</a> to take a final decision about the dataset.</p>
<p>This simple statistical framework is very useful for discovering possible problems in your models when testing and training sets have some differences. Do not forget to check the assumptions for each statistical test and please remember this is just a tool that can provide you with some insights about possible problems in your data. Nothing more and nothing less. ⚠️Please use with caution ⚠️.</p>
<p>For more complete posts about this approach you can check <a href="https://towardsdatascience.com/how-to-compare-two-distributions-in-practice-8c676904a285">here</a> and <a href="https://www.kdnuggets.com/2019/05/sample-huge-dataset-machine-learning.html">also here</a></p>
<p>Finally, you can check other distance measures between two histograms <a href="https://stats.stackexchange.com/questions/7400/how-to-assess-the-similarity-of-two-histograms">here</a></p>
</div>
<div id="the-algoritmic-approach" class="section level3">
<h3>The Algoritmic Approach</h3>
<p>When projecting many predictors into two dimensions such as the case of PCA, intricate predictor relationships could mask regions within the predictor space where the model will inadequately predict new samples. As an alternative you can try the algorithmic described by <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Hastie et al.</a> to get more detailed information about the similarities between two datasets.
The general idea is to create a new dataset randomly permuting the predictors from the training set and then, row-wise concatenate to the original dataset set labeling original and permuted samples. You should check categorical variables was not permuted with a numerical one. Also is a good idea to apply some sort of normalization to the dataset before permuting the variables. Then, a classification model is run on the resulting dataset to predict the probability of new data being in the class of the original set. The benefit of this algorithmic approach is that easy to obtain an error number between the two datasets considered.</p>
<figure>
<img src="/post/2020-11-17-comparing-two-sample-distribution.en_files/hastie-algorithm.png" alt="sampling" width="80%"/>
<figcaption>
<h6>
Figure 2. Pseudo code of the algorithm for building a model to get probability of a new data point is being similar of a given dataset. Taken from Max Khun’s Applied Predictive modeling book
</h6>
</figcaption>
</figure>
<p>Below is a simple <code>R</code> implementation I provide you as-is 😉</p>
<pre><code>library(randomForest)
library(tidyr)
library(dplyr)

efron_simil&lt;-function(train,prec_len){
  train&lt;-train %&gt;% select(1:prec_len)
  predictor_order&lt;-sample(1:prec_len,prec_len)
  train_permuted&lt;-train[,predictor_order]
  names(train_permuted)&lt;-names(train)
  train_permuted$dataset&lt;- &quot;random&quot;
  train$dataset&lt;- &quot;original&quot;
  train&lt;-rbind(train_permuted,train)
  train_model&lt;-randomForest::randomForest(x=train[,1:10],
  y=as.factor(train$dataset))
}

calculate_efron_simil &lt;- function(df_train, df_test,class) {
  results &lt;- list()
  for (i in 1:25) {
    train &lt;- df_train[[i]]
    test &lt;- df_test[[i]] 
    model &lt;- efron_simil(train,ncol(train %&gt;% select(-class)))
    predictions_prob &lt;- predict(model, test, type = &quot;prob&quot;)
    predictions &lt;- ifelse(predictions_prob[,2]&gt;0.5,&quot;random&quot;,&quot;original&quot;)
    results &lt;-
      rbind(
        results,
        table(predictions) %&gt;% as.data.frame() %&gt;% 
          tibble::add_column(repetition =   i)
      )
  }
  results %&gt;% tidyr::pivot_wider(names_from = predictions, 
  values_from = Freq) %&gt;%
    mutate(err = 1-(original / (original + random))) %&gt;% select(err) %&gt;%
    mutate(mean = mean(err), sd = sd(err))
}</code></pre>
</div>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/experimental-design/"> experimental-design </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/cross-validation/"> cross-validation </a>
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/dimensional-reduction/"> dimensional-reduction </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/hastie/"> hastie </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/reproducible-research/"> reproducible-research </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/resampling/"> resampling </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div></div>

  </main>

<footer>

<a href="https://labsin.org">
<img id="labsin-logo-white" class="labsin-logo" src="https://harpomaxx.github.io/img/labsin-logo.svg" alt="LABSIN - Intelligent Systems Laboratory" style="float:left;" />
</a>

<form style="float:center;" action="https://tinyletter.com/harpomaxx" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/harpomaxx', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<label for="tlemail">Enter your email address</label>
<input type="text" style="width:140px" name="email" id="tlemail" />
<input type="hidden" value="1" name="embed"/>
<input type="submit" value="Subscribe" />
</form>


	 © Copyright [Harpo MAxx] MIT LICENSE 
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-175437292-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	


</footer>

</body>
</html>
