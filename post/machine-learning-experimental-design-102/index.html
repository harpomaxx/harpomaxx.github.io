<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.73.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Harpo MAxx"><meta name="keywords" content="cross-validation,resampling,reproducible Research"><meta name="description" content="The usual approach of shuffling the data and split in train and test could not be the best strategy for some cases for getting a good error estimator  of your model. Sometimes Pure random splits do not guarantee the required level of dissimilarity. In adition, all the precautions to avoid contamination of your test set during your trainset manipulation must be taken for the test fold during cross validation. [4min read]"><meta property="og:title" content="Machine Learning Experimental Design 102" />
<meta property="og:description" content="The usual approach of shuffling the data and split in train and test could not be the best strategy for some cases for getting a good error estimator  of your model. Sometimes Pure random splits do not guarantee the required level of dissimilarity. In adition, all the precautions to avoid contamination of your test set during your trainset manipulation must be taken for the test fold during cross validation. [4min read]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harpomaxx.github.io/post/machine-learning-experimental-design-102/" />
<meta property="article:published_time" content="2020-09-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-09-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Experimental Design 102"/>
<meta name="twitter:description" content="The usual approach of shuffling the data and split in train and test could not be the best strategy for some cases for getting a good error estimator  of your model. Sometimes Pure random splits do not guarantee the required level of dissimilarity. In adition, all the precautions to avoid contamination of your test set during your trainset manipulation must be taken for the test fold during cross validation. [4min read]"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/all.css" />
	<link rel="stylesheet" href="https://harpomaxx.github.io/css/katex.min.css" crossorigin="anonymous">
	<script defer src="https://harpomaxx.github.io/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="https://harpomaxx.github.io/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Machine Learning Experimental Design 102 | Computer Science Notes</title></head>
<body><header>
	
	<div id="avatar">
		<a href="https://harpomaxx.github.io/">
			<img src="/img/harpo-2.jpg" alt="Computer Science Notes">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://harpomaxx.github.io/">Computer Science Notes</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href="https://harpomaxx.github.io">CS Notes</a> is a simple blog to keep track about CS-related stuff I consider useful.</p><div id=social>
			<nav>
				<ul><li><a href="https://github.com/harpomaxx/"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="https://www.twitter.com/harpolabs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.researchgate.net/profile/Carlos_Catania"><i title="Research Gate" class="icons fab fa-researchgate"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">22</span>
				<span class="rest">Sep 2020</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Machine Learning Experimental Design 102</h1>
		</div>
	</div>
	<div class="author">
	
	
		<p>
		
		by <a href="https://twitter.com/harpomaxx">Harpo MAxx</a>
		
	
	
		(4 min read)
	
		</p>
	
	</div>

	<div class="markdown">
		


<p>This post is part of a series of posts about experimental design in machine learning: <a href="https://harpomaxx.github.io/post/experimental-design/">Experimental design 101</a></p>
<div id="the-problem-behind-the-split." class="section level3">
<h3>The problem behind the split.</h3>
<p>Shuffling your dataset and split in train/test is not always the best approach. Train and test datasets need some degree of similarity (both need to follow the same distribution), otherwise it would impossible for the model to achieve a decent performance on test (see <a href="https://machinelearningmastery.com/the-model-performance-mismatch-problem/">the model performance mismatch</a> problem). But, on the other hand, if the examples are too similar in both datasets, then you could not assure a real generalization of your model. See <a href="https://stats.stackexchange.com/questions/321295/test-and-training-dataset-correlation-while-splitting-the-dataset">this post</a>
with a simple and straighforward example of wrong random splits.</p>
<p>Obviously, a good practice is to <a href="https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data">visualize the distribution</a> of both (train and test) datasets. In particular the target variable distribution. However, there are some other alternatives to analyze the difference between both datasets. I run into several practical strategies, which I enumerate as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The <a href="https://topepo.github.io/caret/">caret package</a> for <a href="https://www.r-project.org/about.html">R language</a> implements two types of split: the first uses the distribution of your target variable (dependent variable). For classification problem, you can have a certain guarantee that all your classes will be present in your testset. However, all the examples present in testset could be very similar to train and their correct class prediction could be straightforward. Alternatively, the second is based on the information provided by <a href="https://topepo.github.io/caret/data-splitting.html#predictors">the descriptor variables</a>. For creating sub–samples using a maximum dissimilarity caret implements the approach published in <a href="http://www.liebertonline.com/doi/abs/10.1089/106652799318382">Willett,1999</a>. On the other hand, this last split strategy has the problem of not guaranteeing all the classes are present in your test set. As you can see, both approaches have their pro and cons and you probably will need to implement something in between.</p></li>
<li><p>An <a href="https://towardsdatascience.com/how-dis-similar-are-my-train-and-test-data-56af3923de9b">interesting and straightforward</a> technique for analyzing the differences between your train and test set is to run a model for predicting whether a given example belongs to train or test sets. If the model shows high accuracy in classifying each example to the its dataset, then we can be sure they are quite different.</p></li>
<li><p>Another interesting approach used for classification is to do a regression analysis using the labels from training to predict the labels from test (and vice versa). Then you can use the coefficient of determination <span class="math inline">\(R^2\)</span> to analyze the correlation between both datasets.</p></li>
<li><p>If you are performing some sort of cross validation, it is interesting to remember the approach suggested in the <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">ESL</a> quoted below:</p></li>
</ol>
<blockquote>
<p>An obvious, but often ignored, dictum is that <span class="math inline">\(Err_{cval}\)</span> is more believable if
the test set is further separated from the training set. “Further” has a clear
meaning in studies with a time or location factor, but not necessarily in
general. For J -fold cross-validation, separation is improved by removing
contiguous blocks of <span class="math inline">\(N/J\)</span> cases for each group, rather than by random
selection, but the amount of separation is still limited, making <span class="math inline">\(Err_{ccv}\)</span> less
believable than a suitably constructed <span class="math inline">\(Err_{cval}\)</span>.</p>
</blockquote>
</div>
<div id="be-caution-when-using-cross-validationcv" class="section level3">
<h3>Be caution when using Cross Validation(CV)</h3>
<p>As stated in <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">ESL</a>, CV provides you with an reasonable good estimator of your model performance for unseen examples. However, such estimation can suffer when you apply a transformation to your <em>TRAINING</em> data. The figure below shows the classical experimental design followed when you apply some kind of transformation to your <em>TRAINING</em> dataset. An example of such transformation could be scaling/centering your data using the maximum value for some variable present in training dataset or (UP/DOWN)sampling for dealing with unbalanced classes. In any case, it is important that the portion of your training set used for validating during the CV
remains in it original state (i.e. without any transformation). Otherwise, the CV report would not provide you with accurate information and in some cases you could finish with not necessarily the best model. (Note that stablishing the best model is a whole different problem 😱 )</p>
<figure>
<img src="/post/2020-09-14-machine-learning-experimental-design-102.en_files/ml-experimenta-designB.png" alt="experimental design" width="100%"/>
<figcaption>
<h6>
Figure 1. Standard experimental design for evaluating the performance of a machine learning model using some kind of data transformation.
</h6>
</figcaption>
</figure>
<p>Again the <a href="https://topepo.github.io/caret/">caret package</a> for <a href="https://www.r-project.org/about.html">R language</a> provides you with a set of tools for dealing with such situation. In particular, caret discuses the <a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html">problem of down/up sampling inside resampling</a> and according to the documentation, an estimator closer to the real model performance in test is obtained when avoiding any transformation in the k-fold used for validation.</p>
<blockquote>
<p><strong>To sum up, the same precautions you have to avoid being influenced by test set information should be followed for the k-fold used for validation during the CV. Otherwise, the whole model selection process could be affected..
</strong></p>
</blockquote>
<!-- https://stats.stackexchange.com/questions/138569/why-is-lambda-within-one-standard-error-from-the-minimum-is-a-recommended-valu/138573 -->
</div>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/experimental-design/"> experimental-design </a>
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/statistics/"> statistics </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/cross-validation/"> cross-validation </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/reproducible-research/"> reproducible-research </a>
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/resampling/"> resampling </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div></div>

  </main>

<footer>
	 © Copyright [Harpo MAxx] MIT LICENSE 
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-175437292-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
<a href="https://labsin.org">
<img id="labsin-logo-white" class="labsin-logo" src="https://harpomaxx.github.io/img/labsin-logo.svg" alt="LABSIN - Intelligent Systems Laboratory" style="float:left;" />
</a>
</footer>


</body>
</html>
