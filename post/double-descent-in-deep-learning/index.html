<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.73.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Harpo MAxx"><meta name="keywords" content="overfitting"><meta name="description" content="The U-shape observed when measuring model performance on testset as a function of its flexibility does not hold during training deep learning models. (WHAT!!!?????. Is the world going mad?. Not really.)"><meta property="og:title" content="Double Descent in Deep Learning" />
<meta property="og:description" content="The U-shape observed when measuring model performance on testset as a function of its flexibility does not hold during training deep learning models. (WHAT!!!?????. Is the world going mad?. Not really.)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/double-descent-in-deep-learning/" />
<meta property="article:published_time" content="2020-08-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-08-20T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Double Descent in Deep Learning"/>
<meta name="twitter:description" content="The U-shape observed when measuring model performance on testset as a function of its flexibility does not hold during training deep learning models. (WHAT!!!?????. Is the world going mad?. Not really.)"/>
<link rel="icon" type="image/png" href="../../favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="../../favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="../../css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="../../css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="../../css/all.css" />
	<link rel="stylesheet" href="../../css/katex.min.css" crossorigin="anonymous">
	<script defer src="../../js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="../../js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Double Descent in Deep Learning | Computer Science Notes</title></head>
<body><header>
	
	<div id="avatar">
		<a href="../../">
			<img src="../../img/harpo-2.jpg" alt="Computer Science Notes">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="../../">Computer Science Notes</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href="https://harpomaxx.github.io">CS Notes</a> is a simple blog to keep track about CS-related stuff I consider useful.</p><div id=social>
			<nav>
				<ul><li><a href="https://github.com/harpomaxx/"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="https://www.twitter.com/harpolabs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.researchgate.net/profile/Carlos_Catania"><i title="Research Gate" class="icons fab fa-researchgate"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="../../">Home</a></li>
				
				<li><a href="../../post">All Posts</a></li>
				
				<li><a href="../../about">About</a></li>
				
				<li><a href="../../tags">Tags</a></li>
				
				<li><a href="../../categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="author">
	
	</div>
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">20</span>
				<span class="rest">Aug 2020</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Double Descent in Deep Learning</h1>
		</div>
	</div>
	<div class="markdown">
		<p>The classical U-Shape observed when analyzing the performance (usually some loss function) of a model on testset is a fundamental property that holds regardless of the particular data set at hand and regardless of the statistical method being used.</p>
<img src="../../post/2020-08-13-double-descent-in-deep-learning.en_files/u-shape.png" alt="u-shape" width="60%"/>
<p>However, recently Deep Learning research has observed that as they give more flexibility to a deep learning model (usually the size), the performance first gets worse and then gets better. This phenomenon is denoted as Deep Double Descent, which is somewhat surprising.
Last weekend, I ran into a lovely explanation from a statistical point of view of the Double descent phenomeon from <a href="https://twitter.com/daniela_witten">@daniela_witten</a> one of the authors of the well-known <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf">ISLR</a> book.</p>
<p>I put here some resources related with the issue:</p>
<ol>
<li>An article from <a href="https://openai.com/blog/deep-double-descent/">OPENAI</a> about the observed Double Descent in CNN and Restnet networks.</li>
<li>One of the <a href="https://arxiv.org/pdf/1912.02292.pdf">research paper</a> describing the Double Descent phenomenon.</li>
<li>An statistical explanation of the phenomeon from <a href="https://twitter.com/daniela_witten">@daniela_witten</a> in the form of a twitter thread.




<style type="text/css">
  .twitter-tweet {
  font: 14px/1.45 -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;
  border-left: 4px solid #2b7bb9;
  padding-left: 1.5em;
  color: #555;
}
  .twitter-tweet a {
  color: #2b7bb9;
  text-decoration: none;
}
  blockquote.twitter-tweet a:hover,
  blockquote.twitter-tweet a:focus {
  text-decoration: underline;
}
</style>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Bias-Variance Trade-Off &amp; &quot;DOUBLE DESCENT&quot; ðŸ§µ<br><br>Remember the bias-variance trade-off? It says that models  perform well for an &quot;intermediate level of flexibility&quot;.  You&#39;ve seen the picture of the U-shape test error curve.<br><br>We try to hit the &quot;sweet spot&quot; of flexibility.<br><br>1/ðŸ§µ <a href="https://t.co/HPk05izkZh">pic.twitter.com/HPk05izkZh</a></p>&mdash; Daniela Witten (@daniela_witten) <a href="https://twitter.com/daniela_witten/status/1292293102103748609?ref_src=twsrc%5Etfw">August 9, 2020</a></blockquote>
</li>
</ol>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			<a href="../../categories/deep-learning/"> deep-learning </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="../../tags/overfitting/"> overfitting </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div></div>

  </main>

<footer>
	 Â© Copyright [Harpo MAxx] MIT LICENSE 
	
	
	
<a href="https://labsin.org">
<img id="labsin-logo-white" class="labsin-logo" src="https://harpomaxx.github.io/img/labsin-logo.svg" alt="LABSIN - Intelligent Systems Laboratory" style="float:left;" />
</a>
</footer>


</body>
</html>
