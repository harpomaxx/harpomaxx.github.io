<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>random forest on Computer Science Notes</title>
		<link>https://harpomaxx.github.io/tags/random-forest/</link>
		<description>Recent content in random forest on harpomaxx.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Â© Copyright [Harpo MAxx] MIT LICENSE</copyright>
		<lastBuildDate>Fri, 18 Feb 2022 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="https://harpomaxx.github.io/tags/random-forest/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Tackling the limitations of tree-based algorithms</title>
			<link>https://harpomaxx.github.io/post/random-forest-extrapolation/</link>
			<pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/random-forest-extrapolation/</guid>
			<description>
				
				Tree-based algorithms suffer from severe limitations when applied to forecasting problems. They can&#39;t predict beyond observed training data points values. However, not everything is lost. There are some alternative approaches to improve the performance of the tree-based algorithm under such scenarios. [5min read]
				
			</description>
		</item>
		<item>
			<title>SHAP values with examples applied to a multi-classification problem.</title>
			<link>https://harpomaxx.github.io/post/shap-values/</link>
			<pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/shap-values/</guid>
			<description>
				
				We can not continue treating our models as black boxes anymore. Remember, nobody trusts computers for making a very important decision (yet!). That&#39;s why the interpretation of  Machine Learning models has become a major research topic. SHAP is a very robust approach for providing interpretability to any machine learning model.  For multi-classification problems, however, documentation and examples are not very clear. [8min read]
				
			</description>
		</item>
		<item>
			<title>How confident is Random Forest about its predictions?</title>
			<link>https://harpomaxx.github.io/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/</link>
			<pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/</guid>
			<description>
				
				Given a prediction on a particular example, how sure is Random Forest about it? For answering this question it is necessary to look beyond usual performance metrics and dive into the swampy waters of the confidence interval estimation for statistical learning algorithms ðŸ˜–. [6 min read]
				
			</description>
		</item>
		<item>
			<title>Deploying a simple ML model with Plumber 101</title>
			<link>https://harpomaxx.github.io/post/deploying-ml-model/</link>
			<pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/deploying-ml-model/</guid>
			<description>
				
				Sometimes notebooks are not enough and you will need to deploy your machine learning model into company infrastructre. The task involves a lot of Software Ingenieering knowledge, BUT with Plumber package for R you can do the basics with not so much pain ðŸ˜‰. [6 min read]
				
			</description>
		</item>
		<item>
			<title>Feature Selection Strategies</title>
			<link>https://harpomaxx.github.io/post/feature-selection-strategies/</link>
			<pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/feature-selection-strategies/</guid>
			<description>
				
				Feature selection is a topic any machine learning practicioner should master. There are plenty strategies for performing feature selection. Some more useful than others. Some with more limitation than benefits.  Here, I mention the most common approaches for feature selection using information collected from articles, books and research papers. [5 min read]
				
			</description>
		</item>
		<item>
			<title>Features Selection Resources</title>
			<link>https://harpomaxx.github.io/post/features-selection-resources/</link>
			<pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/features-selection-resources/</guid>
			<description>
				
				Beware of Random Forest GINI index for feature importance. Some other resources related with feature selection such as how to use PCA and the problems (or not) behind colinearity.
				
			</description>
		</item>
		
	</channel>
</rss>
