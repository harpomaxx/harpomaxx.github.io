<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Statistics on Computer Science Notes</title>
		<link>/categories/statistics/</link>
		<description>Recent content in Statistics on </description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Â© Copyright [Harpo MAxx] MIT LICENSE</copyright>
		<lastBuildDate>Tue, 03 Aug 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>How confident is Random Forest about its predictions?</title>
			<link>/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/</link>
			<pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
			
			<guid>/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/</guid>
			<description>
				
				Given a prediction on a particular example, how sure is Random Forest about it? For answering this question it is necessary to look beyond usual performance metrics and dive into the swampy waters of the confidence interval estimation for statistical learning algorithms ðŸ˜–. [6 min read]
				
			</description>
		</item>
		<item>
			<title>Inference with Observational Data</title>
			<link>/post/inference-observational/</link>
			<pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate>
			
			<guid>/post/inference-observational/</guid>
			<description>
				
				Despite selection and information bias it is possible to do inference from non-randomized experiments?.  The good ol&#39; statistics comes to help us with its strong theoretical framework. [6min read]
				
			</description>
		</item>
		<item>
			<title>Machine Learning Experimental Design 102</title>
			<link>/post/machine-learning-experimental-design-102/</link>
			<pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
			
			<guid>/post/machine-learning-experimental-design-102/</guid>
			<description>
				
				The usual approach of shuffling the data and split in train and test could not be the best strategy for some cases for getting a good error estimator  of your model. Sometimes pure random splits do not guarantee the required level of dissimilarity. In adition, all the precautions to avoid contamination of your test set during your trainset manipulation must be taken for the test fold during cross validation. [4min read]
				
			</description>
		</item>
		<item>
			<title>Machine Learning Experimental Design 101</title>
			<link>/post/experimental-design/</link>
			<pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
			
			<guid>/post/experimental-design/</guid>
			<description>
				
				Experimental Design in Machine learning is well established. However, from time to time it is important to revisit the process to analyze the  confidence level you have in your results. Machine learning shares a lot with statistics, but since Machine learning practitioners have a more practical vision, sometimes the experimental design is neglected when applied to real-world problems. This note explains the basic strategy followed in almost any machine learning experimental setup.
				
			</description>
		</item>
		<item>
			<title>Statistical Rethinking Book [and other resources]</title>
			<link>/post/statistical-rethinking-book/</link>
			<pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
			
			<guid>/post/statistical-rethinking-book/</guid>
			<description>
				
				A book about Bayesian thinking with examples in R and STAN [and other resources]
				
			</description>
		</item>
		
	</channel>
</rss>
