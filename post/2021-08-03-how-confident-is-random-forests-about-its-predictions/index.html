<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.101.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Harpo MAxx"><meta name="keywords" content="random forest,resampling,efron,hastie"><meta name="description" content="Given a prediction on a particular example, how sure is Random Forest about it? For answering this question it is necessary to look beyond usual performance metrics and dive into the swampy waters of the confidence interval estimation for statistical learning algorithms ðŸ˜–. [6 min read]"><meta property="og:title" content="How confident is Random Forest about its predictions?" />
<meta property="og:description" content="Given a prediction on a particular example, how sure is Random Forest about it? For answering this question it is necessary to look beyond usual performance metrics and dive into the swampy waters of the confidence interval estimation for statistical learning algorithms ðŸ˜–. [6 min read]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harpomaxx.github.io/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-08-03T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-08-03T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How confident is Random Forest about its predictions?"/>
<meta name="twitter:description" content="Given a prediction on a particular example, how sure is Random Forest about it? For answering this question it is necessary to look beyond usual performance metrics and dive into the swampy waters of the confidence interval estimation for statistical learning algorithms ðŸ˜–. [6 min read]"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/all.css" />
	<link rel="stylesheet" href="https://harpomaxx.github.io/css/katex.min.css" crossorigin="anonymous">
	<script defer src="https://harpomaxx.github.io/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="https://harpomaxx.github.io/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>How confident is Random Forest about its predictions? | Computer Science Notes</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/styles/default.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</head>
<body><header>
	
	<div id="avatar">
		<a href="https://harpomaxx.github.io/">
			<img src="/img/harpo-2.jpg" alt="Computer Science Notes">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://harpomaxx.github.io/">Computer Science Notes</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href="https://harpomaxx.github.io">CS Notes</a> is a simple blog to keep track about CS-related stuff I consider useful.</p><div id=social>
			<nav>
				<ul><li><a href="https://github.com/harpomaxx/"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="https://www.twitter.com/harpolabs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.researchgate.net/profile/Carlos_Catania"><i title="Research Gate" class="icons fab fa-researchgate"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">03</span>
				<span class="rest">Aug 2021</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">How confident is Random Forest about its predictions?</h1>
		</div>
	</div>
	<div class="author">
	
	
		<p>
		
		by <a href="https://twitter.com/harpomaxx">Harpo MAxx</a>
		
	
	
		(6 min read)
	
		</p>
	
	</div>

	<div class="markdown">
		
<script src="https://harpomaxx.github.io/post/2021-08-03-how-confident-is-random-forests-about-its-predictions/index.en_files/header-attrs/header-attrs.js"></script>


<p>The problem is quite simple: you have collected a good tabular dataset. After cleaning and doing the <a href="https://harpomaxx.github.io/post/experimental-design/">usual Machine Learning procedure,</a> you decide to try the good olâ€™ Random Forest from our friend <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-4/Remembering-Leo-Breiman/10.1214/10-AOAS427.full">Leo</a>. You train the model and test it on unseen data and surprisingly you got some very decent results.</p>
<p>But now, you asked yourself: given a prediction on a particular example, how sure is Random Forest about it. Or if you put it in different words: if we change the training dataset just a little bit, will Random Forest l give you the same result for that particular example?</p>
<p>You should notice this question is a little different from the usual Machine learning issue regarding model future performance estimation. The traditional question is more focused on how different will be the overall performance of a given model if we use a different dataset. (read about average generalization error <a href="https://harpomaxx.github.io/post/2021-05-14-papers-of-the-week-19-2021/">here</a>). Despite their similarities, the key here is the word <strong>overall</strong>. We are not interested in the overall performance but in a particular example in our test dataset. For dealing with the former, you will use approaches such as K-fold cross-validation or the classic <a href="https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179b">Monte Carlo split</a>. For the latter, there are several approaches, but clearly, the term Confidence interval (CI) arises as a possible solution.</p>
<blockquote>
<h2 id="q-given-a-prediction-on-a-particular-example-how-sure-is-random-forest-about-it" style="color:gray"><strong><em>Q:</em></strong> <strong>Given a prediction on a particular example, how sure is Random Forest about it?</strong></h2>
</blockquote>
<div id="confidence-intervals" class="section level2">
<h2>Confidence intervals</h2>
<p>Since Random Forest (RF) outputs an estimation of the class probability, it is possible to calculate confidence intervals. Confidence intervals will provide you with a possible â€˜margin of errorâ€™ of the output probability class. So, letâ€™s say RF output for a given example is 0.60. The usual approach is to assign that example to the positive class (&gt;0.5). But, what would happen if you obtain a confidence interval of 0.15. That means that the probability range will lay between 0.45 and 0.65 (with a certain alpha level). And consequently, RF has effectively abandoned any hope of unambiguously classifying the example. On the other hand, for a CI value of 0.05 then, the probability range will lay between 0.55 and 0.65, and under such a scenario, the classification will be always positive for that given example. To sum up, the lower the CI the more confident you will be about RF predictions.</p>
</div>
<div id="a-practical-example" class="section level2">
<h2>A practical example</h2>
<p>Letâ€™s try an example using a dataset containing information about malicious and normal network traffic connections.</p>
<p>You do the usual 70/30 split in train and test sets and then you train four different models using Random Forest varying the number of trees. In the end, you get the following table:</p>
<table>
<caption>Table 1: Prediction results for RF using different numbers of trees.</caption>
<thead>
<tr class="header">
<th align="center">Number of Trees</th>
<th align="center">Accuracy</th>
<th align="center">Sensitivity</th>
<th align="center">Specificity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">50</td>
<td align="center">0.969</td>
<td align="center">0.990</td>
<td align="center">0.720</td>
</tr>
<tr class="even">
<td align="center">100</td>
<td align="center">0.960</td>
<td align="center">0.986</td>
<td align="center">0.655</td>
</tr>
<tr class="odd">
<td align="center">500</td>
<td align="center">0.964</td>
<td align="center">0.987</td>
<td align="center">0.702</td>
</tr>
<tr class="even">
<td align="center">1000</td>
<td align="center">0.969</td>
<td align="center">0.987</td>
<td align="center">0.703</td>
</tr>
</tbody>
</table>
<p>The problem here (notice it is oversimplified) is which model you will choose. Accuracy and Sensitivity are practically the same. An even for specificity you have all but the model with 100 trees showing the same values. If you follow a traditional policy of selecting the best value. Then, RF with <em>ntree = 50</em> will be the model of choice. However, if you look at the CI distribution for each of the output probabilities, things could be different.</p>
<p>In figure 1. shows a plot of test-set predictions against estimates of standard error (<span class="math inline">\(se\)</span>) for all four RF models. Orange dots refer to instances incorrectly classified while blue to correctly classified. The solid curves are smoothing splines fit through the data. Remember that <span class="math inline">\(CI = mean\pm se\)</span> .</p>
<p>Now, you can see some observable differences between the four RF models. The model using 50 trees have a very large <span class="math inline">\(se\)</span> values, in some cases very close to 1. So it basically means that the model is not sure about <strong>anything</strong> and the results from Table 1 should be taking so seriously. Moreover, It is possible that the model is overfitting the data. On the other hand, the model using 1000 trees and all <span class="math inline">\(se\)</span> values under 0.25, it is certainly more confident about its predictions.</p>
<figure>
<img src="images/rf_ci.png" alt="roles" width="100%"/>
  <figcaption>
      <h6> Figure 1. Predictions against estimates of standard error ($se$) for all four RF models.   </h6>
  </figcaption>
</figure>
<p>Notice the results in Figure 1 are consistent with the way RF works for binary classification problems. If you use more trees, you are reducing the variance of the of de model and the RF will be more confident with its own predictions. An intuition about this is given in <a href="https://stats.stackexchange.com/questions/348245/do-we-have-to-tune-the-number-of-trees-in-a-random-forest">this article</a>. The article states that for each sample, the ensemble will tend toward a particular mean prediction value for that sample as the number of trees tends towards infinity.</p>
</div>
<div id="under-the-hood" class="section level2">
<h2>Under the hood</h2>
<p>So, how is CI calculated for random forests?, there are several approaches (see the list of papers at the end of the post). But the one I used for the above example is based on the article <a href="Confidence%20Intervals%20for%20Random%20Forests:%20The%20Jackknife%20and%20the%20Infinitesimal%20Jackknife">Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife (Wager, Hastie and Efron 2014)</a></p>
<p>The wager approach uses the <a href="https://en.wikipedia.org/wiki/Jackknife_resampling">Jackknife</a> resampling technique for estimating the CI. Jackknife was proposed by <a href="https://en.wikipedia.org/wiki/John_Tukey" title="John Tukey">John Tukey</a> in 1956. The general idea behind Jackknife is that a parameter estimate can be found by estimating the parameter for each subsample omitting the <em>i</em>-th observation.</p>
<p>More formally, you have to generate a jackknife sample that has the value <span class="math inline">\(x_i\)</span> removed and then compute the <em>i</em>-th partial estimate of the statistic using this sample,</p>
<p><span class="math display">\[
\hat{\theta_{i}}(x_1,...,x_{x-1},x_n)
\]</span></p>
<p>We then turn this <em>i</em>-th partial estimate into the <em>i</em>-th pseudo value <span class="math inline">\(\hat{\theta}_{i}*\)</span> using the following equation</p>
<p><span class="math display">\[
\hat{\theta}_{i}*=n\hat{\theta} - (n -1) \hat{\theta}_{i}
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}*\)</span> is the estimate using the full dataset.</p>
<p>Letâ€™s see an example in R. We have a vector <span class="math inline">\(x\)</span> and we define the function <code>tetha</code> that corresponds to the coefficient of variation (i.e the statistics we want to estimate)</p>
<pre class="r"><code>x &lt;-c(8.26, 6.33, 10.4, 5.27, 5.35, 5.61, 6.12, 6.19, 5.2,
7.01, 8.74, 7.78, 7.02, 6, 6.5, 5.8, 5.12, 7.41, 6.52, 6.21,
12.28, 5.6, 5.38, 6.6, 8.74)

theta &lt;- function(x) sqrt(var(x))/mean(x)</code></pre>
<p>We create two vectors, <code>jack</code> and <code>pseudo</code> and we need to fill in the elements of the jack sample vector as follows. For <span class="math inline">\(j&lt;i\)</span>, the <span class="math inline">\(j âˆ’ 1\)</span>th element of <code>jack</code> is the <em>d</em> th element of <em>d</em>. We can state all this using a logical <code>if .. else</code> statement within a<code>for</code> loop,</p>
<pre class="r"><code>jack &lt;- numeric(length(x)-1)
pseudo &lt;- numeric(length(x))

for (i in 1:length(x)) {
  for (j in 1:length(x)) {
    if(j &lt; i) jack[j] &lt;- x[j]
      else if(j &gt; i) jack[j-1] &lt;- x[j]
  }
  pseudo[i] &lt;- length(x)*theta(x) -(length(x)-1)*theta(jack)
}</code></pre>
<p>Now we can calculate approximate 95% confidence interval by calculating the upper and lower limits</p>
<pre class="r"><code>mean(pseudo) + qt(0.975,length(x)-1)*sqrt(var(pseudo)/length(x))</code></pre>
<pre><code>## [1] 0.3729806</code></pre>
<pre class="r"><code>mean(pseudo) - qt(0.975,length(x)-1)*sqrt(var(pseudo)/length(x))</code></pre>
<pre><code>## [1] 0.1504947</code></pre>
<p>Giving the approximate 95% jackknife confidence interval as 0.150 to 0.372. Please note that this is an oversimplified explanation of the jackknife technique. The method described in Wagerâ€™s article is clearly more advance, but I think this could help you (and me) to understand the basics of the approach.</p>
</div>
<div id="some-final-words" class="section level2">
<h2>Some final wordsâ€¦</h2>
<p>The Jackknife method as any other resampling method is computationally expensive and could be difficult to apply to the large datasets used nowadays.</p>
<p>Please keep in mind that here you are not dealing with prediction intervals (i.e., intervals that cover new observations <span class="math inline">\(Y_{i}\)</span> with high probability), instead, you are dealing with confidence intervals (i.e., intervals that cover <span class="math inline">\(\mu(x) = E[Y|X=x]\)</span> with high probability. In case you need the former, using <a href="https://jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">Quantile regression approaches</a> seems to be the way to go.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://github.com/imbs-hl/ranger/">[1]</a> The <code>ranger</code> package for R is a fast implementation of Random forest that includes the Wager approach for confidence intervals.</p>
<p><a href="https://jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">[2]</a> Quantile Regression Forests provides a method for prediction intervals.</p>
<p><a href="http://www.jmlr.org/papers/volume17/14-168/14-168.pdf">[3]</a> The U-statistics approach of <a href="http://www.jmlr.org/papers/volume17/14-168/14-168.pdf">Mentch and Hooker (2006)</a> is another approach for confidence intervals.<br />
<a href="https://github.com/imbs-hl/ranger/issues/136">[4]</a> A discussion about implementation details is available on Github.</p>
<p><a href="http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/references/R-bootstrap.pdf">[5]</a> Professor Chen has some notes discussing the basics of several resampling methods using for estimating statistics. I stole my example from him</p>
</div>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/algorithms/"> algorithms </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/papers/"> papers </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/statistics/"> statistics </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/efron/"> efron </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/hastie/"> hastie </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/random-forest/"> random-forest </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/resampling/"> resampling </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div></div>

  </main>

<footer>

<a href="https://labsin.org">
<img id="labsin-logo-white" class="labsin-logo" src="https://harpomaxx.github.io/img/labsin-logo.svg" alt="LABSIN - Intelligent Systems Laboratory" style="float:left;" />
</a>

<form style="float:center;" action="https://tinyletter.com/harpomaxx" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/harpomaxx', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<label for="tlemail">Enter your email address</label>
<input type="text" style="width:140px" name="email" id="tlemail" />
<input type="hidden" value="1" name="embed"/>
<input type="submit" value="Subscribe" />
</form>


	 Â© Copyright [Harpo MAxx] MIT LICENSE 
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-175437292-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	


</footer>

</body>
</html>
