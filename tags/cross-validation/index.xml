<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>cross-validation on Computer Science Notes</title>
		<link>https://harpomaxx.github.io/tags/cross-validation/</link>
		<description>Recent content in cross-validation on harpomaxx.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Â© Copyright [Harpo MAxx] MIT LICENSE</copyright>
		<lastBuildDate>Sun, 04 Sep 2022 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="https://harpomaxx.github.io/tags/cross-validation/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Beyond Random Split for Assessing Statistical Model Performance</title>
			<link>https://harpomaxx.github.io/post/beyond-random-split-for-assessing-statistical-model-performance/</link>
			<pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/beyond-random-split-for-assessing-statistical-model-performance/</guid>
			<description>
				
				Sometimes  the standard splitting techniques used for testing your machine learning models can underestimate the generalization performance of the model. In this  post, I expose some of the most common approaches for splitting your data beyond the classical random split approach. [5min read]
				
			</description>
		</item>
		<item>
			<title>Thoughts about differences in ML evaluation for Academia and Industry</title>
			<link>https://harpomaxx.github.io/post/2021-05-14-papers-of-the-week-19-2021/</link>
			<pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/2021-05-14-papers-of-the-week-19-2021/</guid>
			<description>
				
				The processeses and the methods followed in Academia for evaluating a Machine Learning Model are different from the approaches used by the Industry. Why? [4min read]
				
			</description>
		</item>
		<item>
			<title>Three Common Ways for Comparing Two Dataset Distributions</title>
			<link>https://harpomaxx.github.io/post/comparing-two-sample-distribution/</link>
			<pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/comparing-two-sample-distribution/</guid>
			<description>
				
				From time to time you will need to compare the distribution of two datasets. There are plenty of information about this topic in statistics books and all over the Internet. In this post I discuss three very practical approaches coming from different perspectives. [3.5 min read](updated 04/01/2021)
				
			</description>
		</item>
		<item>
			<title>Machine Learning Experimental Design 102</title>
			<link>https://harpomaxx.github.io/post/machine-learning-experimental-design-102/</link>
			<pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/machine-learning-experimental-design-102/</guid>
			<description>
				
				The usual approach of shuffling the data and split in train and test could not be the best strategy for some cases for getting a good error estimator  of your model. Sometimes pure random splits do not guarantee the required level of dissimilarity. In adition, all the precautions to avoid contamination of your test set during your trainset manipulation must be taken for the test fold during cross validation. [4min read]
				
			</description>
		</item>
		<item>
			<title>Machine Learning Experimental Design 101</title>
			<link>https://harpomaxx.github.io/post/experimental-design/</link>
			<pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/experimental-design/</guid>
			<description>
				
				Experimental Design in Machine learning is well established. However, from time to time it is important to revisit the process to analyze the  confidence level you have in your results. Machine learning shares a lot with statistics, but since Machine learning practitioners have a more practical vision, sometimes the experimental design is neglected when applied to real-world problems. This note explains the basic strategy followed in almost any machine learning experimental setup.
				
			</description>
		</item>
		
	</channel>
</rss>
