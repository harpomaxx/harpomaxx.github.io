<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>language models on Computer Science Notes</title>
		<link>https://harpomaxx.github.io/categories/language-models/</link>
		<description>Recent content in language models on harpomaxx.github.io</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Â© Copyright [Harpo MAxx] MIT LICENSE</copyright>
		<lastBuildDate>Mon, 01 Apr 2024 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="https://harpomaxx.github.io/categories/language-models/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Converting your own finetuned language model to GGUF</title>
			<link>https://harpomaxx.github.io/post/2024-04-01-converting-to-gguf/</link>
			<pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
			
			<guid>https://harpomaxx.github.io/post/2024-04-01-converting-to-gguf/</guid>
			<description>
				
				Sometimes your finetuned language Model work as expected but you need a faster inference time. Some other times you need to reduce memory footprint. By transformring your models to the GGUF format you can store quantized models and using them on top of the fast llama.cpp inference engine. [5 min read]
				
			</description>
		</item>
		
	</channel>
</rss>
