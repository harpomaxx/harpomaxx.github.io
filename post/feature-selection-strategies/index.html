<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.80.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="Harpo MAxx"><meta name="keywords" content="random forest,lasso,tibshirani,hastie"><meta name="description" content="Feature selection is a topic any machine learning practicioner should master. There are plenty strategies for performing feature selection. Some more useful than others. Some with more limitation than benefits.  Here, I mention the most common approaches for feature selection using information collected from articles, books and research papers. [5 min read]"><meta property="og:title" content="Feature Selection Strategies" />
<meta property="og:description" content="Feature selection is a topic any machine learning practicioner should master. There are plenty strategies for performing feature selection. Some more useful than others. Some with more limitation than benefits.  Here, I mention the most common approaches for feature selection using information collected from articles, books and research papers. [5 min read]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harpomaxx.github.io/post/feature-selection-strategies/" />
<meta property="article:published_time" content="2020-12-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-14T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Feature Selection Strategies"/>
<meta name="twitter:description" content="Feature selection is a topic any machine learning practicioner should master. There are plenty strategies for performing feature selection. Some more useful than others. Some with more limitation than benefits.  Here, I mention the most common approaches for feature selection using information collected from articles, books and research papers. [5 min read]"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://harpomaxx.github.io/css/all.css" />
	<link rel="stylesheet" href="https://harpomaxx.github.io/css/katex.min.css" crossorigin="anonymous">
	<script defer src="https://harpomaxx.github.io/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="https://harpomaxx.github.io/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Feature Selection Strategies | Computer Science Notes</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/styles/default.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.1/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
</head>
<body><header>
	
	<div id="avatar">
		<a href="https://harpomaxx.github.io/">
			<img src="/img/harpo-2.jpg" alt="Computer Science Notes">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://harpomaxx.github.io/">Computer Science Notes</a></h2></div>
	<div id="title-description"><p id="subtitle"><a href="https://harpomaxx.github.io">CS Notes</a> is a simple blog to keep track about CS-related stuff I consider useful.</p><div id=social>
			<nav>
				<ul><li><a href="https://github.com/harpomaxx/"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="https://www.twitter.com/harpolabs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.researchgate.net/profile/Carlos_Catania"><i title="Research Gate" class="icons fab fa-researchgate"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">14</span>
				<span class="rest">Dec 2020</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Feature Selection Strategies</h1>
		</div>
	</div>
	<div class="author">
	
	
		<p>
		
		by <a href="https://twitter.com/harpomaxx">Harpo MAxx</a>
		
	
	
		(5 min read)
	
		</p>
	
	</div>

	<div class="markdown">
		
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Selecting the attributes used in your model is a fundamental aspect of the <a href="https://machinelearningmastery.com/an-introduction-to-feature-selection/">machine learning workflow</a>. It would be really great if we just could use all the information from our problem without performing any kind of selection. Unfortunately, the intrinsic definition of information is basically the problem. We don’t have information, all we have is just data. Noisy, dirty and incomplete data. So the question is: From all the data we have, what are the variables that provide us valuable information and what are the variables that are basically garbage?</p>
<p>A classical article discussing aspects of feature selection was written by <a href="https://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf">Guyon et al. in 2003</a>. With more than 16000 citations, this article is definitively <strong>A MUST READ!</strong>. The <a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a> book is another good source of information about feature selection. The topic is discussed in Chapter 19.</p>
<p>There basically three categories under features selection algorithms can be included. This is a well known-classification, however I stolen this classification from this <a href="https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2">excelent article about feature selection</a>.</p>
<div id="filter-based" class="section level2">
<h2>Filter based</h2>
<p>In filter based feature selection, we specify some metric and based on that metric we filter features. Two well-known approaches falling under this category are Pearson’s correlation and chi-square.</p>
<p>When using correlation, we simply filter/remove highly correlated features <a href="https://towardsdatascience.com/feature-selection-in-machine-learning-d5af31f7276">considering their absolute value</a>.
Here we can left only one of the correlated features or combine all the correlated features in one using something like <span class="math inline">\(new\_feature=0.5*corfeat1+0.5*corfeat2+corfeat3*0.5...\)</span>.</p>
<p>In the case of chi-square, the idea is to <a href="https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223#:~:text=In%20feature%20selection%2C%20we%20aim,hypothesis%20of%20independence%20is%20incorrect.">apply an
statistical test to select those features</a> that have the strongest relationship with the output variable (they are highly dependent on the response). The same idea can be applied to any other statistical test. The sckit-learn library provide a wrapper function <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest">selectKBest</a> that facilitates the use of <a href="https://en.wikipedia.org/wiki/Analysis_of_variance">ANOVA</a>, <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared</a> tests among others. Notice this is an <a href="https://en.wikipedia.org/wiki/Univariate_analysis">univariate</a> feature selection approach. In other words, the relation between variables are not considered. Consequently, there could be a case where two variables do not show dependence with response when analyzed separately, but when both are present in our data, they results in fundamental features for the problem (See <a href="https://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf">Guyon et al, 2003</a>).</p>
<p>Another interesting approach for selecting features is based on the application of <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>. Here the idea is to establish the importance of each feature based on the magnitude of the corresponding values in the eigenvectors (higher magnitude - higher importance). Let’s say hypothetically we have 3 PCs (Principal Components) that explain more than 70% of the variability of our data. We can choose the features with the higher magnitude on the most important PCs. I have my personal concerns about using PCA for feature selection since the if you don’t have a high variability explained in only a few PCs, it would be difficult to measures which are the best features on each PC. Anyway, here it is a post <a href="https://stackoverflow.com/questions/50796024/feature-variable-importance-after-a-pca-analysis">discussing the idea</a> and a <a href="https://www.researchgate.net/profile/Robert_Gao/publication/3090942_PCA-Based_Feature_Selection_Scheme_for_Machine_Defect_Classification/links/0deec52a9ab734d4f5000000/PCA-Based-Feature-Selection-Scheme-for-Machine-Defect-Classification.pdf">research article</a> using this approach.</p>
</div>
<div id="wrapper-based" class="section level2">
<h2>Wrapper-based</h2>
<p>These methods consider the selection of a set of features as a search problem. Notice that under these strategies types, some sort of model/algorithm is required for conducting the search process. The idea is to use the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. The classic strategy following this approach is the so called Recursive Feature Elimination (RFE). The goal of RFE is to select features by recursively considering smaller and smaller sets of features.(<a href="https://link.springer.com/article/10.1023/A:1012487302797">see Isabelle Guyon’s paper about RFE</a>). A multitude of modern search procedures can be applied to the feature selection problem to reduce the computational requirements of the search process. Two of the most common procedure are discussed in the excellent book (<a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a>) are <a href="https://en.wikipedia.org/wiki/Simulated_annealing">Simulated Annealing</a> and <a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3#:~:text=A%20genetic%20algorithm%20is%20a,offspring%20of%20the%20next%20generation.">Genetic Algorithms</a>.</p>
<p>As mentioned in <a href="https://en.wikipedia.org/wiki/Feature_selection#Wrapper_method">wikipedia</a>, the two main disadvantages of these methods are:</p>
<ol style="list-style-type: decimal">
<li>The increasing overfitting risk when the number of observations is insufficient.</li>
<li>The significant computation time when the number of variables is large.</li>
</ol>
<p>To deal with 1. it is important to follow an appropriate <a href="https://harpomaxx.github.io/post/experimental-design/">machine learning methodology</a>. Using resampling as well as an independent test evaluation (train-CV and test). In the case of 2. some sort of distributed computation techniques will be required.</p>
</div>
<div id="embedded-based" class="section level2">
<h2>Embedded Based:</h2>
<p>Embedded methods use algorithms that have built-in feature selection methods. Two well-known algorithms that include some sort of internal feature selection are [Lasso Regression](<a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" class="uri">https://en.wikipedia.org/wiki/Lasso_(statistics)</a>, a sort of linear regression a with penalization term popularized by <a href="https://statweb.stanford.edu/~tibs/lasso/lasso.pdf">Robert Tibshirani in the late 90s</a> and the good ol’ Random Forests.</p>
<p>In the case of Lasso Regression, the algorithm includes a penalization term that can reduce some coefficient values to zero. As a result, models generated from the lasso consider only the most important variables and are generally much easier to interpret. So we can extract the variables with coefficients different from zero to perform a straightforward feature selection.</p>
<p>Random Forests (RF) apply the mean decrease in impurity (MDI or gini importance) mechanism. The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs. Once the model was fit, we can extract the gini importance index from all the variables and select the top N for our model.</p>
</div>
<div id="extra-bonus" class="section level2">
<h2>Extra Bonus</h2>
<p>This excellent <a href="https://explained.ai/rf-importance/#:~:text=The%20advantage%20of%20Random%20Forests,to%20the%20feature%20importance%20function.&amp;text=Notice%20that%20the%20function%20does,dividing%20by%20the%20standard%20deviation.">article</a> (I’ve already mentioned it in other posts), discusses some drawbacks with RF importance mechanism, and mention an alternative <strong>WRAPPER</strong> approach for measuring the importance of not only RF but also any other machine learning algorithm. The main idea is to randomly permute the predictor colummns. The importance of that feature is the difference between the baseline and the drop in overall accuracy or any other considered metric caused by permuting the column. The authors of the previous article have extended this idea and proposed <a href="https://arxiv.org/abs/2006.04750">a non-parametric approach</a> for feature selection based on semi partial curves.</p>
<p>Recently, I have found other articles discussing the importance of permutation for feature selection.</p>
<ul>
<li><p>This <a href="https://towardsdatascience.com/a-relook-on-random-forest-and-feature-importance-2467dfab5cca">particular article</a> compares both approaches and point me to <a href="https://cran.r-project.org/web/packages/Boruta/index.html">Boruta</a>, an R library for performing feature selection based on permutation (also <a href="https://github.com/scikit-learn-contrib/boruta_py">available</a> for Python).</p></li>
<li><p>In this <a href="https://arxiv.org/abs/2001.04295">article</a>, the authors discuss more formally the problems and limitation behind the MDI approach.</p></li>
</ul>
</div>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/feature-importance/"> feature-importance </a>
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/categories/feature-selection/"> feature-selection </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Tags</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/hastie/"> hastie </a>
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/lasso/"> lasso </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/random-forest/"> random-forest </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="https://harpomaxx.github.io/tags/tibshirani/"> tibshirani </a>
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
		
	</div></div>

  </main>

<footer>

<a href="https://labsin.org">
<img id="labsin-logo-white" class="labsin-logo" src="https://harpomaxx.github.io/img/labsin-logo.svg" alt="LABSIN - Intelligent Systems Laboratory" style="float:left;" />
</a>

<form style="float:center;" action="https://tinyletter.com/harpomaxx" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/harpomaxx', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<label for="tlemail">Enter your email address</label>
<input type="text" style="width:140px" name="email" id="tlemail" />
<input type="hidden" value="1" name="embed"/>
<input type="submit" value="Subscribe" />
</form>


	 © Copyright [Harpo MAxx] MIT LICENSE 
	
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-175437292-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	


</footer>

</body>
</html>
